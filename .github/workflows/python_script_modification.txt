
# Add this new argument and functionality to proxy_scraper.py

# In the parse_arguments function, add:
parser.add_argument('--combine-individual', 
                   action='store_true',
                   help='Create combined proxy list from individual files')

# Add this new function to the ProxyScraper class:
def combine_individual_lists(self):
    """Combine individual proxy list files into a single combined file"""
    import glob

    combined_proxies = []
    proxy_files = glob.glob('proxylist-*.jdproxies')

    if not proxy_files:
        logger.warning("No individual proxy files found to combine")
        return

    logger.info(f"Combining {len(proxy_files)} individual proxy files...")

    for file_path in proxy_files:
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
                proxies = data.get('customProxyList', [])
                combined_proxies.extend(proxies)
                logger.info(f"Added {len(proxies)} proxies from {file_path}")
        except Exception as e:
            logger.error(f"Error reading {file_path}: {e}")

    # Remove duplicates
    seen = set()
    unique_proxies = []

    for proxy in combined_proxies:
        try:
            proxy_key = f"{proxy['proxy']['address']}:{proxy['proxy']['port']}"
            if proxy_key not in seen:
                seen.add(proxy_key)
                unique_proxies.append(proxy)
        except (KeyError, TypeError):
            continue

    # Sort by response time
    unique_proxies.sort(key=lambda x: x.get('response_time', 999))

    # Save combined list
    self.save_proxies(unique_proxies)

    logger.info(f"Combined {len(unique_proxies)} unique proxies from {len(combined_proxies)} total")

# In the main() function, add this logic:
if args.combine_individual:
    scraper = ProxyScraper([])  # Empty list since we're just combining
    scraper.combine_individual_lists()
    return
